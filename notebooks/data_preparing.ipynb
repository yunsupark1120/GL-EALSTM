{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAGE II Modeling - Data Preparation\n",
    "\n",
    "-------------------\n",
    "### Author: Yunsu Park\n",
    "### Date: 2025-01-23\n",
    "---------------------\n",
    "\n",
    "This notebook prepares the data for the GAGE II modeling. \n",
    "\n",
    "To reproduce the result of this notebook, make sure the the following folders are in the same directory as this notebook:\n",
    "\n",
    "- `GL_daymet_1980-2023`: contains the climate variables for the GAGE II modeling \n",
    "- `discharge/Canada`: a __folder__ containing the discharge data for the Canadian stations\n",
    "- `discharge/GL_USA_USGS_streamflow.csv`: a __file__ containing the discharge data for the US stations\n",
    "\n",
    "All of the data listed above are available in the project Googld Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Basin Data Preparation\n",
    "\n",
    "## Data Cleaning for discharge variable\n",
    "\n",
    "This following code cell processes multiple CSV files containing discharge data, cleans the data, and exports the cleaned data to a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:22<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_discharge_data(input_folder: str, output_folder: str):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a complete date range from '1980/01/01' to '2023/12/31'\n",
    "    date_range = pd.date_range(start='1980-01-01', end='2023-12-31')\n",
    "    date_range_df = pd.DataFrame(date_range, columns=['Date'])\n",
    "    \n",
    "    # Iterate through all CSV files in the input folder\n",
    "    for file_name in tqdm(os.listdir(input_folder)):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path, skiprows=1)\n",
    "            \n",
    "            # Filter rows where PARAM is 1\n",
    "            df_filtered = df[df['PARAM'] == 1]\n",
    "            \n",
    "            # Group by ID and save each group to a separate CSV file\n",
    "            for gage_id, group in df_filtered.groupby(' ID'):\n",
    "                # Convert the 'Date' column to datetime\n",
    "                group['Date'] = pd.to_datetime(group['Date'], format='%Y/%m/%d')\n",
    "                \n",
    "                # Merge with the complete date range\n",
    "                merged_df = pd.merge(date_range_df, group[['Date', 'Value']], on='Date', how='left')\n",
    "                merged_df.rename(columns={'Value': 'discharge'}, inplace=True)\n",
    "                \n",
    "                # Export the merged data\n",
    "                output_file_path = os.path.join(output_folder, f\"{gage_id.strip()}.csv\")\n",
    "                merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'discharge/Canada'\n",
    "output_folder = 'Canadian_basins_discharge_cleaned'\n",
    "process_discharge_data(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine climate data with discharge data\n",
    "\n",
    "Now that the discharge data is sorted out, we can combine the discharge data with the climate data. \n",
    "\n",
    "The following code cell reads the discharge data and the climate data, and combines them into a single dataframe. The combined dataframe is then exported to a specified directory.\n",
    "\n",
    "Also, since the climate data is availalbe from 1980 to 2023, we will only use the discharge data from 1980 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Define the directories\n",
    "discharge_dir = r\"Canadian_basins_discharge_cleaned\"\n",
    "climate_dir = r\"GL_daymet_1980-2023\"\n",
    "output_dir = r\"Canadian_climate_discharge_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all discharge files and extract basin IDs\n",
    "discharge_files = glob(os.path.join(discharge_dir, \".csv\"))\n",
    "basin_ids = [os.path.basename(f).split('_')[1].split('.')[0] for f in discharge_files]\n",
    "\n",
    "print(basin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 577/577 [04:50<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directories\n",
    "discharge_dir = r\"Canadian_basins_discharge_cleaned\"\n",
    "climate_dir = r\"GL_daymet_1980-2023\"\n",
    "output_dir = r\"Canadian_climate_discharge_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all discharge files and extract basin IDs\n",
    "discharge_files = glob(os.path.join(discharge_dir, \"*.csv\"))\n",
    "basin_ids = [os.path.basename(f).split('.')[0] for f in discharge_files]\n",
    "\n",
    "# Loop through each basin ID\n",
    "for basin_id in tqdm(basin_ids, desc=\"Processing basins\"):\n",
    "    # Read the discharge data\n",
    "    discharge_file = os.path.join(discharge_dir, f\"{basin_id}.csv\")\n",
    "    discharge_data = pd.read_csv(discharge_file)\n",
    "    discharge_data['Date'] = pd.to_datetime(discharge_data['Date'], format='%Y-%m-%d')\n",
    "    discharge_data.rename(columns={'Value': 'discharge'}, inplace=True)\n",
    "    \n",
    "    # Initialize a list to hold climate data\n",
    "    climate_data_list = []\n",
    "    \n",
    "    # Loop through each subfolder (variable) in the climate directory\n",
    "    for subfolder in os.listdir(climate_dir):\n",
    "        subfolder_path = os.path.join(climate_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Check if the climate file exists for the current basin ID\n",
    "            climate_file = os.path.join(subfolder_path, f\"{basin_id}_daymet_{subfolder.split('_')[-1]}.csv\")\n",
    "            if os.path.exists(climate_file):\n",
    "                # Read the climate data\n",
    "                climate_data = pd.read_csv(climate_file)\n",
    "                climate_data['time'] = pd.to_datetime(climate_data['time']).dt.strftime('%Y-%m-%d')\n",
    "                climate_data.rename(columns={'time': 'Date', subfolder.split('_')[-1]: subfolder.split('_')[-1]}, inplace=True)\n",
    "                climate_data['Date'] = pd.to_datetime(climate_data['Date'], format='%Y-%m-%d')\n",
    "                climate_data_list.append(climate_data)\n",
    "    \n",
    "    # Merge all climate data with discharge data on 'Date'\n",
    "    combined_data = discharge_data\n",
    "    for climate_data in climate_data_list:\n",
    "        combined_data = pd.merge(combined_data, climate_data, on='Date', how='inner')\n",
    "    \n",
    "    # Drop rows with dates earlier than 1980/01/01\n",
    "    combined_data = combined_data[combined_data['Date'] >= '1980/01/01']\n",
    "    \n",
    "    # Export the combined data\n",
    "    output_file = os.path.join(output_dir, f\"{basin_id}.csv\")\n",
    "    combined_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the processed data has all the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through CSV files is done.\n",
      "All files contain necessary columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the necessary columns\n",
    "necessary_columns = ['Date', 'discharge', 'dayl', 'prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp'] \n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'Canadian_climate_discharge_data'\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for file_name in tqdm(os.listdir(folder_path)):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        missing_columns = [col for col in necessary_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            results.append(f\"{file_name} is missing columns: {', '.join(missing_columns)}\")\n",
    "        \n",
    "\n",
    "# Print the result message\n",
    "print(\"Looping through CSV files is done.\")\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"All files contain necessary columns.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Basins\n",
    "\n",
    "## Combine climate data with discharge data\n",
    "\n",
    "The US discharge data is available in a single CSV file. \n",
    "\n",
    "Therefore, we can directly combine the discharge data with the climate data, skipping the data sorting process as we did for the Canadian basins.\n",
    "\n",
    "The following code cell reads the discharge data and the climate data, and combines them into a single dataframe per basin. The combined dataframes are then exported to a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 399/399 [04:46<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directories\n",
    "discharge_file = r\"discharge\\GL_USA_USGS_streamflow.csv\"\n",
    "climate_dir = r\"GL_daymet_1980-2023\"\n",
    "output_dir = r\"US_climate_discharge_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the discharge data\n",
    "discharge_data = pd.read_csv(discharge_file)\n",
    "discharge_data['datetime'] = pd.to_datetime(discharge_data['datetime'])\n",
    "discharge_data['Date'] = discharge_data['datetime'].dt.strftime('%Y/%m/%d')\n",
    "discharge_data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "# Get all basin IDs from the discharge data columns\n",
    "basin_ids = discharge_data.columns.drop('Date')\n",
    "\n",
    "# Loop through each basin ID\n",
    "for basin_id in tqdm(basin_ids, desc=\"Processing basins\"):\n",
    "    # Extract the discharge data for the current basin ID\n",
    "    basin_discharge_data = discharge_data[['Date', basin_id]].rename(columns={basin_id: 'discharge'})\n",
    "    basin_discharge_data['Date'] = pd.to_datetime(basin_discharge_data['Date'], format='%Y/%m/%d')\n",
    "    \n",
    "    # Initialize a list to hold climate data\n",
    "    climate_data_list = []\n",
    "    \n",
    "    # Loop through each subfolder (variable) in the climate directory\n",
    "    for subfolder in os.listdir(climate_dir):\n",
    "        subfolder_path = os.path.join(climate_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Check if the climate file exists for the current basin ID\n",
    "            climate_file = os.path.join(subfolder_path, f\"{basin_id}_daymet_{subfolder.split('_')[-1]}.csv\")\n",
    "            if os.path.exists(climate_file):\n",
    "                # Read the climate data\n",
    "                climate_data = pd.read_csv(climate_file)\n",
    "                climate_data['time'] = pd.to_datetime(climate_data['time']).dt.strftime('%Y/%m/%d')\n",
    "                climate_data.rename(columns={'time': 'Date', subfolder.split('_')[-1]: subfolder.split('_')[-1]}, inplace=True)\n",
    "                climate_data['Date'] = pd.to_datetime(climate_data['Date'], format='%Y/%m/%d')\n",
    "                climate_data_list.append(climate_data)\n",
    "    \n",
    "    # Merge all climate data with discharge data on 'Date'\n",
    "    combined_data = basin_discharge_data\n",
    "    for climate_data in climate_data_list:\n",
    "        combined_data = pd.merge(combined_data, climate_data, on='Date', how='inner')\n",
    "    \n",
    "    # Drop rows with dates earlier than 1980/01/01\n",
    "    combined_data = combined_data[combined_data['Date'] >= '1980/01/01']\n",
    "    \n",
    "    # Export the combined data\n",
    "    output_file = os.path.join(output_dir, f\"{basin_id}.csv\")\n",
    "    combined_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the processed data has all the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [00:07<00:00, 55.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through CSV files is done.\n",
      "All files contain necessary columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the necessary columns\n",
    "necessary_columns = ['Date', 'discharge', 'dayl', 'prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp'] \n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'US_climate_discharge_data'\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for file_name in tqdm(os.listdir(folder_path)):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        missing_columns = [col for col in necessary_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            results.append(f\"{file_name} is missing columns: {', '.join(missing_columns)}\")\n",
    "        \n",
    "\n",
    "# Print the result message\n",
    "print(\"Looping through CSV files is done.\")\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"All files contain necessary columns.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
