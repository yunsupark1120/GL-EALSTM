{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8525b331",
   "metadata": {},
   "source": [
    "# Study Area Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49befc5",
   "metadata": {},
   "source": [
    "## 1. Setup (Run this first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fe105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. COMMON DATA LOADING AND PREPARATION (US & Canada) - CORRECTED ID COLS ---\n",
      "Found 976 studied basin IDs from CSV files.\n",
      "Attributes CSV loaded: 1088 rows.\n",
      "Attributes data filtered to 976 studied basins.\n",
      "US Shapefile 'shapefiles\\GL_GAGE2_all.shp' loaded with 439 features. CRS: PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "US GDF processed. Length: 439. Target ID col: 'gauge_id_shp'\n",
      "Canadian Shapefile 'shapefiles\\ADP02_Basin_Select.shp' loaded with 649 features. CRS: PROJCS[\"Canada_Albers_Equal_Area_Conic\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",40],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",50],PARAMETER[\"standard_parallel_2\",70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102001\"]]\n",
      "Canadian GDF processed. Length: 649. Target ID col: 'gauge_id_shp'\n",
      "US GDF added to list for concatenation. CRS: PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "Reprojecting Canadian GDF from PROJCS[\"Canada_Albers_Equal_Area_Conic\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",40],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",50],PARAMETER[\"standard_parallel_2\",70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102001\"]] to PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]] for concatenation.\n",
      "Canadian GDF reprojected. New CRS: PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "Canadian GDF added to list for concatenation.\n",
      "Combined shapefiles. Total 1088 features. Initial Combined CRS: PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "Number of unique gauge_id_shp in combined GDF: 1088\n",
      "Merging gdf_all_countries (gauge_id_shp) with attributes_study_df (gauge_id)\n",
      "Merged combined shapefile with attributes, resulting in 976 features for study.\n",
      "Reprojecting final GDF from PROJCS[\"NAD_1983_Albers\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]] to EPSG:4326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ybrot\\AppData\\Local\\Temp\\ipykernel_7364\\26428224.py:139: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  study_domain_boundary = gdf_study.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant land cover calculated. Counts:\n",
      " dominant_lc\n",
      "Agriculture    361\n",
      "Forest         339\n",
      "Open Water     208\n",
      "Urban           56\n",
      "Wetland         12\n",
      "Name: count, dtype: int64\n",
      "Using MANUALLY set map extent: [-94, -73, 39.5, 50]\n",
      "--- COMMON DATA PREPARATION COMPLETE (US & Canada) - CORRECTED ID COLS ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D # For custom legends\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# For map projections and features\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "print(\"--- 1. COMMON DATA LOADING AND PREPARATION (US & Canada) - CORRECTED ID COLS ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DIR = '../metadata/'\n",
    "ATTRIBUTES_PATH = os.path.join(BASE_DIR, 'attributes.csv')\n",
    "CSV_FILTERED_DIR = os.path.join(BASE_DIR, 'csv_filtered')\n",
    "\n",
    "# CORRECTED Shapefile ID Column Names\n",
    "US_SHAPEFILE_PATH = os.path.join('shapefiles', 'GL_GAGE2_all.shp')\n",
    "US_SHP_GAUGE_ID_COL = 'GAGE_ID' # As per your clarification for US stations\n",
    "\n",
    "CA_SHAPEFILE_PATH = os.path.join('shapefiles', 'ADP02_Basin_Select.shp')\n",
    "CA_SHP_GAUGE_ID_COL = 'StationNum' # VERIFY this is correct for Canadian. If it's 'StationName', change it.\n",
    "\n",
    "COLS_TO_LOAD_ATTR = [\n",
    "    'gauge_id', 'area', 'pre_mm_syr', 'tmp_dc_syr', 'dis_m3_pyr',\n",
    "    'for_pc_sse', 'crp_pc_sse', 'urb_pc_sse', 'wet_pc_sg1', 'lka_pc_sse',\n",
    "]\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "basin_csv_files = glob.glob(os.path.join(CSV_FILTERED_DIR, '*.csv'))\n",
    "studied_basin_ids = [os.path.splitext(os.path.basename(f))[0] for f in basin_csv_files]\n",
    "print(f\"Found {len(studied_basin_ids)} studied basin IDs from CSV files.\")\n",
    "\n",
    "try:\n",
    "    attributes_df = pd.read_csv(ATTRIBUTES_PATH, usecols=COLS_TO_LOAD_ATTR)\n",
    "    attributes_df['gauge_id'] = attributes_df['gauge_id'].astype(str).str.strip()\n",
    "    print(f\"Attributes CSV loaded: {len(attributes_df)} rows.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading attributes CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "attributes_study_df = attributes_df[attributes_df['gauge_id'].isin(studied_basin_ids)].copy()\n",
    "print(f\"Attributes data filtered to {len(attributes_study_df)} studied basins.\")\n",
    "if attributes_study_df.empty:\n",
    "    print(\"No matching basins found between attributes.csv and filtered CSV files. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- Load US Shapefile ---\n",
    "gdf_us = None\n",
    "try:\n",
    "    gdf_us_raw = gpd.read_file(US_SHAPEFILE_PATH)\n",
    "    print(f\"US Shapefile '{US_SHAPEFILE_PATH}' loaded with {len(gdf_us_raw)} features. CRS: {gdf_us_raw.crs}\")\n",
    "    if US_SHP_GAUGE_ID_COL not in gdf_us_raw.columns:\n",
    "        print(f\"ERROR: Column '{US_SHP_GAUGE_ID_COL}' not found in US shapefile. Available: {gdf_us_raw.columns.tolist()}\")\n",
    "    else:\n",
    "        gdf_us = gdf_us_raw[[US_SHP_GAUGE_ID_COL, 'geometry']].copy()\n",
    "        gdf_us = gdf_us.rename(columns={US_SHP_GAUGE_ID_COL: 'gauge_id_shp'}) # Standardize to 'gauge_id_shp'\n",
    "        gdf_us['gauge_id_shp'] = gdf_us['gauge_id_shp'].astype(str).str.strip()\n",
    "        print(f\"US GDF processed. Length: {len(gdf_us)}. Target ID col: 'gauge_id_shp'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing US Shapefile: {e}\")\n",
    "\n",
    "# --- Load Canadian Shapefile ---\n",
    "gdf_ca = None\n",
    "try:\n",
    "    gdf_ca_raw = gpd.read_file(CA_SHAPEFILE_PATH)\n",
    "    print(f\"Canadian Shapefile '{CA_SHAPEFILE_PATH}' loaded with {len(gdf_ca_raw)} features. CRS: {gdf_ca_raw.crs}\")\n",
    "    if CA_SHP_GAUGE_ID_COL not in gdf_ca_raw.columns:\n",
    "        print(f\"ERROR: Column '{CA_SHP_GAUGE_ID_COL}' not found in Canadian shapefile. Available: {gdf_ca_raw.columns.tolist()}\")\n",
    "    else:\n",
    "        gdf_ca = gdf_ca_raw[[CA_SHP_GAUGE_ID_COL, 'geometry']].copy()\n",
    "        gdf_ca = gdf_ca.rename(columns={CA_SHP_GAUGE_ID_COL: 'gauge_id_shp'}) # Standardize to 'gauge_id_shp'\n",
    "        gdf_ca['gauge_id_shp'] = gdf_ca['gauge_id_shp'].astype(str).str.strip()\n",
    "        print(f\"Canadian GDF processed. Length: {len(gdf_ca)}. Target ID col: 'gauge_id_shp'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing Canadian Shapefile: {e}\")\n",
    "\n",
    "# --- Concatenate US and Canadian GeoDataFrames ---\n",
    "# (Rest of the common data preparation block remains the same as the previous \"DEBUGGING\" version)\n",
    "gdf_all_countries_list = []\n",
    "common_crs_for_concat = None\n",
    "\n",
    "if gdf_us is not None and not gdf_us.empty:\n",
    "    common_crs_for_concat = gdf_us.crs\n",
    "    gdf_all_countries_list.append(gdf_us)\n",
    "    print(f\"US GDF added to list for concatenation. CRS: {gdf_us.crs}\")\n",
    "\n",
    "if gdf_ca is not None and not gdf_ca.empty:\n",
    "    if common_crs_for_concat is None:\n",
    "        common_crs_for_concat = gdf_ca.crs\n",
    "    if gdf_ca.crs != common_crs_for_concat:\n",
    "        print(f\"Reprojecting Canadian GDF from {gdf_ca.crs} to {common_crs_for_concat} for concatenation.\")\n",
    "        try:\n",
    "            gdf_ca = gdf_ca.to_crs(common_crs_for_concat)\n",
    "            print(f\"Canadian GDF reprojected. New CRS: {gdf_ca.crs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reprojecting Canadian GDF: {e}. Skipping Canadian data for concatenation.\")\n",
    "            gdf_ca = None\n",
    "    if gdf_ca is not None and not gdf_ca.empty:\n",
    "         gdf_all_countries_list.append(gdf_ca)\n",
    "         print(\"Canadian GDF added to list for concatenation.\")\n",
    "\n",
    "if not gdf_all_countries_list:\n",
    "    print(\"ERROR: No shapefile data available for concatenation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "gdf_all_countries = gpd.GeoDataFrame(pd.concat(gdf_all_countries_list, ignore_index=True), crs=common_crs_for_concat)\n",
    "print(f\"Combined shapefiles. Total {len(gdf_all_countries)} features. Initial Combined CRS: {gdf_all_countries.crs}\")\n",
    "print(f\"Number of unique gauge_id_shp in combined GDF: {gdf_all_countries['gauge_id_shp'].nunique()}\")\n",
    "\n",
    "print(f\"Merging gdf_all_countries (gauge_id_shp) with attributes_study_df (gauge_id)\")\n",
    "gdf_study = gdf_all_countries.merge(attributes_study_df, left_on='gauge_id_shp', right_on='gauge_id', how='inner')\n",
    "print(f\"Merged combined shapefile with attributes, resulting in {len(gdf_study)} features for study.\")\n",
    "\n",
    "# Debugging the merge (uncomment if needed):\n",
    "# common_ids = set(gdf_all_countries['gauge_id_shp'].unique()) & set(attributes_study_df['gauge_id'].unique())\n",
    "# print(f\"Number of common IDs for merge: {len(common_ids)}\")\n",
    "# if len(common_ids) < 10 and len(common_ids) > 0: print(f\"Sample common IDs: {list(common_ids)[:min(10, len(common_ids))]}\")\n",
    "# else:\n",
    "#     print(\"Sample IDs from gdf_all_countries['gauge_id_shp']:\", list(gdf_all_countries['gauge_id_shp'].unique()[:10]))\n",
    "#     print(\"Sample IDs from attributes_study_df['gauge_id']:\", list(attributes_study_df['gauge_id'].unique()[:10]))\n",
    "\n",
    "\n",
    "if gdf_study.empty:\n",
    "    print(\"No features after merging. This means 'gauge_id_shp' from the combined shapefiles did not match 'gauge_id' in your attributes for the studied basins.\")\n",
    "    exit()\n",
    "\n",
    "TARGET_CRS = 'EPSG:4326'\n",
    "if gdf_study.crs != TARGET_CRS:\n",
    "    print(f\"Reprojecting final GDF from {gdf_study.crs} to {TARGET_CRS}\")\n",
    "    gdf_study = gdf_study.to_crs(TARGET_CRS)\n",
    "else:\n",
    "    print(f\"Final GDF already in target CRS: {TARGET_CRS}\")\n",
    "\n",
    "study_domain_boundary = gdf_study.unary_union\n",
    "domain_boundary_gdf = gpd.GeoDataFrame(geometry=[study_domain_boundary], crs=gdf_study.crs)\n",
    "\n",
    "def get_dominant_land_cover(row):\n",
    "    lc_categories = {\n",
    "        'Forest': row.get('for_pc_sse', 0), 'Agriculture': row.get('crp_pc_sse', 0),\n",
    "        'Urban': row.get('urb_pc_sse', 0), 'Wetland': row.get('wet_pc_sg1', 0),\n",
    "        'Open Water': row.get('lka_pc_sse', 0),\n",
    "    }\n",
    "    valid_lc = {k: v for k, v in lc_categories.items() if pd.notna(v)}\n",
    "    if not valid_lc: return 'Unknown'\n",
    "    dominant = max(valid_lc, key=valid_lc.get)\n",
    "    return dominant\n",
    "\n",
    "required_lc_cols = ['for_pc_sse', 'crp_pc_sse', 'urb_pc_sse', 'wet_pc_sg1', 'lka_pc_sse']\n",
    "if all(col in gdf_study.columns for col in required_lc_cols):\n",
    "    gdf_study['dominant_lc'] = gdf_study.apply(get_dominant_land_cover, axis=1)\n",
    "    lc_colors = {\n",
    "        'Forest': 'darkgreen', 'Agriculture': 'gold', 'Urban': 'gray',\n",
    "        'Wetland': 'mediumblue', 'Open Water': 'lightblue', 'Unknown': 'white'\n",
    "    }\n",
    "    print(\"Dominant land cover calculated. Counts:\\n\", gdf_study['dominant_lc'].value_counts())\n",
    "else:\n",
    "    missing_cols_lc = [col for col in required_lc_cols if col not in gdf_study.columns]\n",
    "    print(f\"Skipping dominant land cover (missing columns: {missing_cols_lc}).\")\n",
    "    gdf_study['dominant_lc'] = 'Not Calculated'\n",
    "    lc_colors = {'Not Calculated': 'white'}\n",
    "\n",
    "great_lakes_labels = {\n",
    "    'Lake Superior': (-88.0, 47.5), 'Lake Michigan': (-87.0, 44.0),\n",
    "    'Lake Huron': (-82.5, 44.5), 'Lake Erie': (-81.0, 42.2),\n",
    "    'Lake Ontario': (-77.5, 43.7),\n",
    "}\n",
    "\n",
    "map_extent = [-94, -73, 39.5, 50]\n",
    "print(f\"Using MANUALLY set map extent: {map_extent}\")\n",
    "\n",
    "def setup_great_lakes_map(ax_map, extent):\n",
    "    ax_map.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "    ax_map.add_feature(cfeature.LAND.with_scale('50m'), facecolor='#E0E0E0', edgecolor='gray', zorder=0)\n",
    "    ax_map.add_feature(cfeature.OCEAN.with_scale('50m'), facecolor='aliceblue', zorder=0)\n",
    "    ax_map.add_feature(cfeature.LAKES.with_scale('10m'), facecolor='aliceblue', edgecolor='darkgray', zorder=1)\n",
    "    ax_map.add_feature(cfeature.BORDERS.with_scale('50m'), linestyle=':', edgecolor='black', linewidth=0.7, zorder=2)\n",
    "    ax_map.add_feature(cfeature.STATES.with_scale('50m'), linestyle=':', edgecolor='dimgray', linewidth=0.5, zorder=2)\n",
    "    for name, (lon, lat) in great_lakes_labels.items():\n",
    "        ax_map.text(lon, lat, name, transform=ccrs.Geodetic(),\n",
    "                    ha='center', va='center', fontsize=7, fontweight='bold',\n",
    "                    bbox=dict(facecolor='white', alpha=0.5, pad=0.1, edgecolor='none'), zorder=5)\n",
    "    return ax_map\n",
    "\n",
    "print(\"--- COMMON DATA PREPARATION COMPLETE (US & Canada) - CORRECTED ID COLS ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24def785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gauge_id_shp', 'geometry', 'gauge_id', 'crp_pc_sse', 'wet_pc_sg1',\n",
       "       'for_pc_sse', 'pre_mm_syr', 'tmp_dc_syr', 'urb_pc_sse', 'lka_pc_sse',\n",
       "       'dis_m3_pyr', 'area', 'dominant_lc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_study.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a042ec1",
   "metadata": {},
   "source": [
    "## 2. Figure 1a (This version is not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdf1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATING FIGURE 1: Study Domain and Land Cover (US & Canada) ---\n",
      "Figure 1 saved as Figure1_Study_Domain_Land_Cover_Combined.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- GENERATING FIGURE 1: Study Domain and Land Cover (US & Canada) ---\")\n",
    "\n",
    "# Ensure the common block has been run and gdf_study, domain_boundary_gdf are available\n",
    "if 'gdf_study' not in globals() or 'domain_boundary_gdf' not in globals() or gdf_study.empty: # Added gdf_study.empty check\n",
    "    print(\"ERROR: Common data preparation block not run or gdf_study is empty. Please run it first.\")\n",
    "else:\n",
    "    fig1, (ax1a, ax1b) = plt.subplots(1, 2, figsize=(16, 8), # Width, Height\n",
    "                                   subplot_kw={'projection': ccrs.LambertConformal(central_longitude=-85, central_latitude=45)},\n",
    "                                   constrained_layout=True) # Use constrained_layout\n",
    "\n",
    "    # Panel A: Geographic Location of Study Basins\n",
    "    ax1a = setup_great_lakes_map(ax1a, map_extent)\n",
    "    ax1a.set_title('A) Geographic Location of Studied Basins', fontsize=11)\n",
    "    gdf_study.plot(ax=ax1a, transform=ccrs.PlateCarree(),\n",
    "                   facecolor='skyblue', edgecolor='darkblue', linewidth=0.3, alpha=0.6, zorder=3)\n",
    "    if not domain_boundary_gdf.empty: # Check if domain_boundary_gdf is not empty\n",
    "        domain_boundary_gdf.plot(ax=ax1a, transform=ccrs.PlateCarree(),\n",
    "                                 facecolor='none', edgecolor='red', linewidth=1.2, linestyle='--', zorder=4)\n",
    "    handles_a = [\n",
    "        Line2D([0], [0], marker='s', color='w', label='Studied Basins Extent', markerfacecolor='skyblue', markeredgecolor='darkblue', markersize=8),\n",
    "        Line2D([0], [0], color='red', lw=1.2, linestyle='--', label='Overall Study Domain Boundary')\n",
    "    ]\n",
    "    # Place legend consistently\n",
    "    ax1a.legend(handles=handles_a, loc='lower left', fontsize=8, title_fontsize=9, frameon=False) # Added frameon=False for cleaner look\n",
    "    ax1a.text(0.01, 0.01, '(Basemap: Natural Earth)', transform=ax1a.transAxes, fontsize=6, ha='left', va='bottom', color='dimgray')\n",
    "\n",
    "\n",
    "    # Panel B: Generalized Land Cover of Studied Basins\n",
    "    ax1b = setup_great_lakes_map(ax1b, map_extent)\n",
    "    ax1b.set_title('B) Dominant Land Cover of Studied Basins', fontsize=11)\n",
    "    legend_elements_lc = []\n",
    "    # Ensure consistent order in legend for land cover categories\n",
    "    # Use lc_colors keys for a predefined order if that's desirable,\n",
    "    # otherwise gdf_study['dominant_lc'].unique() is fine but order might vary run-to-run.\n",
    "    # Let's use the order from lc_colors if all are present, otherwise unique values.\n",
    "    \n",
    "    plot_lc_types = []\n",
    "    if 'dominant_lc' in gdf_study.columns and gdf_study['dominant_lc'].nunique() > 1 and gdf_study['dominant_lc'].iloc[0] != 'Not Calculated':\n",
    "        # Prefer order from lc_colors if defined\n",
    "        defined_lc_order = [lc for lc in lc_colors.keys() if lc in gdf_study['dominant_lc'].unique() and lc not in ['Not Calculated', 'Unknown']]\n",
    "        if not defined_lc_order: # Fallback if lc_colors doesn't match unique values well\n",
    "            defined_lc_order = sorted([lc for lc in gdf_study['dominant_lc'].unique() if lc not in ['Not Calculated', 'Unknown']])\n",
    "\n",
    "        for lc_type in defined_lc_order:\n",
    "            color = lc_colors.get(lc_type, 'purple') # Default color if somehow missing from lc_colors\n",
    "            gdf_subset = gdf_study[gdf_study['dominant_lc'] == lc_type]\n",
    "            if not gdf_subset.empty:\n",
    "                gdf_subset.plot(ax=ax1b, transform=ccrs.PlateCarree(),\n",
    "                                facecolor=color, edgecolor='black', linewidth=0.1, label=lc_type, zorder=3)\n",
    "                legend_elements_lc.append(Line2D([0], [0], marker='s', color='w', label=lc_type,\n",
    "                                              markerfacecolor=color, markersize=8))\n",
    "        if legend_elements_lc:\n",
    "            # Place legend consistently\n",
    "            ax1b.legend(handles=legend_elements_lc, title=\"Dominant Land Cover\", loc='lower left', fontsize=8, title_fontsize=9, frameon=False)\n",
    "    else:\n",
    "        gdf_study.plot(ax=ax1b, transform=ccrs.PlateCarree(), facecolor='lightgrey', edgecolor='black', linewidth=0.1, zorder=3)\n",
    "        ax1b.text(0.5, 0.5, \"Dominant Land Cover Not Plotted\\n(Data missing or uniform)\",\n",
    "                  transform=ax1b.transAxes, ha='center', va='center', color='red', fontsize=9)\n",
    "    ax1b.text(0.01, 0.01, '(Basemap: Natural Earth)', transform=ax1b.transAxes, fontsize=6, ha='left', va='bottom', color='dimgray')\n",
    "\n",
    "    # fig1.suptitle(\"Figure 1: Study Domain and Dominant Land Cover\", fontsize=14) # Optional overall title\n",
    "\n",
    "    # No plt.tight_layout() needed if constrained_layout=True is used effectively.\n",
    "    # If constrained_layout=True is not giving desired results, remove it and uncomment plt.tight_layout(pad=...)\n",
    "    # plt.tight_layout(pad=2.0) # Try this if constrained_layout is not satisfactory\n",
    "\n",
    "    plt.savefig('Figure1_Study_Domain_Land_Cover_Combined.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Figure 1 saved as Figure1_Study_Domain_Land_Cover_Combined.png\")\n",
    "    plt.close(fig1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a0e87",
   "metadata": {},
   "source": [
    "## 3. Figure 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ed7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATING FIGURE 2: Climatic Gradients (with F to C conversion) ---\n",
      "Temperature column 'tmp_dc_syr' converted from F to C as 'tmp_dc_syr_C'.\n",
      "Figure 2 saved as Figure2_Climatic_Gradients_Celsius.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Ensure pandas is imported if not already from common block\n",
    "import numpy as np # Ensure numpy is imported if not already from common block\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "# Assuming setup_great_lakes_map, map_extent, and gdf_study are available from the common block\n",
    "\n",
    "print(\"\\n--- GENERATING FIGURE 2: Climatic Gradients (with F to C conversion) ---\")\n",
    "\n",
    "if 'gdf_study' not in globals() or gdf_study.empty:\n",
    "    print(\"ERROR: Common data preparation block not run or gdf_study is empty. Please run it first.\")\n",
    "else:\n",
    "    fig2, (ax2a, ax2b) = plt.subplots(1, 2, figsize=(16, 8),\n",
    "                                   subplot_kw={'projection': ccrs.LambertConformal(central_longitude=-85, central_latitude=45)})\n",
    "\n",
    "    # Create a copy for modifications to avoid SettingWithCopyWarning if gdf_study is a slice\n",
    "    gdf_plot_fig2 = gdf_study.copy()\n",
    "\n",
    "    # Panel A: Mean Annual Precipitation\n",
    "    ax2a = setup_great_lakes_map(ax2a, map_extent) # setup_great_lakes_map should be defined in your common block\n",
    "    ax2a.set_title('A) Mean Annual Precipitation (mm/year)', fontsize=11)\n",
    "    if 'pre_mm_syr' in gdf_plot_fig2.columns:\n",
    "        gdf_plot_fig2.plot(ax=ax2a, transform=ccrs.PlateCarree(), column='pre_mm_syr',\n",
    "                           cmap='Blues', legend=True,\n",
    "                           legend_kwds={'label': \"Precipitation (mm/year)\", 'orientation': \"horizontal\", 'shrink': 0.5, 'pad': 0.1, 'aspect': 30},\n",
    "                           edgecolor='darkgray', linewidth=0.1, missing_kwds={'color': 'lightgrey', \"hatch\": \"///\", \"label\": \"Missing values\"})\n",
    "    else:\n",
    "        ax2a.text(0.5, 0.5, \"Precipitation Data ('pre_mm_syr') Not Found\", transform=ax2a.transAxes, ha='center', va='center', color='red', fontsize=9)\n",
    "    ax2a.text(0.01, 0.01, '(Basemap: Natural Earth)', transform=ax2a.transAxes, fontsize=6, ha='left', va='bottom', color='dimgray')\n",
    "\n",
    "    # Panel B: Mean Annual Air Temperature\n",
    "    ax2b = setup_great_lakes_map(ax2b, map_extent)\n",
    "    ax2b.set_title('B) Mean Annual Air Temperature (°C)', fontsize=11)\n",
    "    if 'tmp_dc_syr' in gdf_plot_fig2.columns:\n",
    "        # Convert Fahrenheit to Celsius\n",
    "        # Ensure the column is numeric before conversion\n",
    "        if pd.api.types.is_numeric_dtype(gdf_plot_fig2['tmp_dc_syr']):\n",
    "            gdf_plot_fig2['tmp_dc_syr_C'] = (gdf_plot_fig2['tmp_dc_syr'] - 32) * 5/9\n",
    "            print(\"Temperature column 'tmp_dc_syr' converted from F to C as 'tmp_dc_syr_C'.\")\n",
    "\n",
    "            gdf_plot_fig2.plot(ax=ax2b, transform=ccrs.PlateCarree(), column='tmp_dc_syr_C', # Plot the new Celsius column\n",
    "                               cmap='coolwarm', legend=True,\n",
    "                               legend_kwds={'label': \"Temperature (°C)\", 'orientation': \"horizontal\", 'shrink': 0.5, 'pad': 0.1, 'aspect': 30},\n",
    "                               edgecolor='darkgray', linewidth=0.1, missing_kwds={'color': 'lightgrey', \"hatch\": \"///\", \"label\": \"Missing values\"})\n",
    "        else:\n",
    "            print(\"ERROR: Temperature column 'tmp_dc_syr' is not numeric. Cannot convert F to C.\")\n",
    "            ax2b.text(0.5, 0.5, \"Temperature Data ('tmp_dc_syr')\\nNot Numeric for Conversion\", transform=ax2b.transAxes, ha='center', va='center', color='red', fontsize=9)\n",
    "    else:\n",
    "        ax2b.text(0.5, 0.5, \"Temperature Data ('tmp_dc_syr') Not Found\", transform=ax2b.transAxes, ha='center', va='center', color='red', fontsize=9)\n",
    "    ax2b.text(0.01, 0.01, '(Basemap: Natural Earth)', transform=ax2b.transAxes, fontsize=6, ha='left', va='bottom', color='dimgray')\n",
    "\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    plt.savefig('Figure2_Climatic_Gradients_Celsius.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Figure 2 saved as Figure2_Climatic_Gradients_Celsius.png\")\n",
    "    plt.close(fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319e578",
   "metadata": {},
   "source": [
    "## 4. Figure 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e3863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATING FIGURE 3: Distribution of Key Basin Characteristics (Using Manually Computed Discharge) ---\n",
      "\n",
      "Plotting histogram for 'mean_discharge_m3s_manual':\n",
      "count    858.000000\n",
      "mean      14.153692\n",
      "std       27.919092\n",
      "min        0.038030\n",
      "25%        1.177993\n",
      "50%        3.595899\n",
      "75%       12.208385\n",
      "max      204.097644\n",
      "Name: mean_discharge_m3s_manual, dtype: float64\n",
      "Figure 3 saved as Figure3_Basin_Characteristics_Distribution_ManualQ.png\n",
      "\n",
      "--- FIGURE 3 GENERATION ATTEMPTED (Using Manually Computed Discharge) ---\n"
     ]
    }
   ],
   "source": [
    "# This code should run AFTER your \"Common Data Loading and Preparation Block\"\n",
    "# AND AFTER the \"Phase 1: Compute Numerical Values\" block (or at least the part\n",
    "# where 'mean_discharge_m3s_manual' is calculated and added to gdf_study).\n",
    "\n",
    "print(\"\\n--- GENERATING FIGURE 3: Distribution of Key Basin Characteristics (Using Manually Computed Discharge) ---\")\n",
    "\n",
    "if 'gdf_study' not in globals() or gdf_study.empty:\n",
    "    print(\"ERROR: Common data preparation block not run or gdf_study is empty. Please run it first.\")\n",
    "elif 'mean_discharge_m3s_manual' not in gdf_study.columns:\n",
    "    print(\"ERROR: 'mean_discharge_m3s_manual' column not found in gdf_study. Ensure manual calculation was run.\")\n",
    "else:\n",
    "    fig3, (ax3a, ax3b) = plt.subplots(1, 2, figsize=(10, 4)) # Smaller figure\n",
    "\n",
    "    # Panel A: Drainage Areas (remains the same, uses 'area' column)\n",
    "    if 'area' in gdf_study.columns:\n",
    "        areas_km2 = gdf_study['area'].dropna()\n",
    "        if not areas_km2.empty:\n",
    "            # Determine if log scale is appropriate based on range/skewness\n",
    "            if (areas_km2.max() / areas_km2.min() > 100) and areas_km2.min() > 0 : # Heuristic for log scale\n",
    "                ax3a.hist(np.log10(areas_km2), bins=25, color='skyblue', edgecolor='black')\n",
    "                ax3a.set_xlabel('Log₁₀(Drainage Area [km²])')\n",
    "            else:\n",
    "                ax3a.hist(areas_km2, bins=25, color='skyblue', edgecolor='black')\n",
    "                ax3a.set_xlabel('Drainage Area (km²)')\n",
    "        else:\n",
    "            ax3a.text(0.5, 0.5, \"Area Data Missing\\nor All NaN\", transform=ax3a.transAxes, ha='center', va='center', color='red')\n",
    "    else:\n",
    "        ax3a.text(0.5, 0.5, \"Area Column Missing\", transform=ax3a.transAxes, ha='center', va='center', color='red')\n",
    "    ax3a.set_ylabel('Number of Basins')\n",
    "    ax3a.set_title('A) Distribution of Drainage Areas', fontsize=10)\n",
    "    ax3a.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax3a.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "\n",
    "    # Panel B: Mean Annual Discharge (using the new 'mean_discharge_m3s_manual' column)\n",
    "    discharge_column_for_plot = 'mean_discharge_m3s_manual'\n",
    "    if discharge_column_for_plot in gdf_study.columns:\n",
    "        # Use the manually calculated mean discharge for each basin\n",
    "        discharge_values_for_plot = gdf_study[discharge_column_for_plot].dropna()\n",
    "\n",
    "        if not discharge_values_for_plot.empty:\n",
    "            print(f\"\\nPlotting histogram for '{discharge_column_for_plot}':\")\n",
    "            print(discharge_values_for_plot.describe()) # See stats of what's being plotted\n",
    "\n",
    "            # Determine if log scale is appropriate\n",
    "            min_q_plot = discharge_values_for_plot.min()\n",
    "            max_q_plot = discharge_values_for_plot.max()\n",
    "\n",
    "            if min_q_plot > 0 and (max_q_plot / min_q_plot > 100): # Heuristic for log scale\n",
    "                ax3b.hist(np.log10(discharge_values_for_plot), bins=25, color='lightcoral', edgecolor='black')\n",
    "                ax3b.set_xlabel('Log₁₀(Mean Annual Discharge [m³/s])')\n",
    "            else:\n",
    "                ax3b.hist(discharge_values_for_plot, bins=25, color='lightcoral', edgecolor='black')\n",
    "                ax3b.set_xlabel('Mean Annual Discharge (m³/s)')\n",
    "        else:\n",
    "            ax3b.text(0.5, 0.5, \"Manually Calculated\\nDischarge Data Missing\\nor All NaN\", transform=ax3b.transAxes, ha='center', va='center', color='red', fontsize=8)\n",
    "    else:\n",
    "        ax3b.text(0.5, 0.5, f\"Column '{discharge_column_for_plot}'\\nMissing from gdf_study\", transform=ax3b.transAxes, ha='center', va='center', color='red', fontsize=8)\n",
    "\n",
    "    ax3b.set_ylabel('Number of Basins')\n",
    "    ax3b.set_title('B) Distribution of Mean Annual Discharge', fontsize=10)\n",
    "    ax3b.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax3b.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "\n",
    "    plt.tight_layout(pad=1.0)\n",
    "    plt.savefig('Figure3_Basin_Characteristics_Distribution_ManualQ.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Figure 3 saved as Figure3_Basin_Characteristics_Distribution_ManualQ.png\")\n",
    "    plt.close(fig3)\n",
    "\n",
    "print(\"\\n--- FIGURE 3 GENERATION ATTEMPTED (Using Manually Computed Discharge) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c399bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MANUALLY CALCULATING MEAN ANNUAL DISCHARGE & COMPUTING STATS ---\n",
      "Calculating mean discharge from daily CSVs...\n",
      "Summary: 118 basins had no valid discharge data or issues during processing.\n",
      "Manual mean discharge calculation complete and merged.\n",
      "Columns before merge: ['gauge_id_shp', 'geometry', 'gauge_id', 'crp_pc_sse', 'wet_pc_sg1', 'for_pc_sse', 'pre_mm_syr', 'tmp_dc_syr', 'urb_pc_sse', 'lka_pc_sse', 'dis_m3_pyr', 'area', 'dominant_lc', 'dis_m3_s', 'tmp_dc_syr_C', 'mean_discharge_m3s_manual_x', 'mean_discharge_m3s_manual_y']\n",
      "Columns after merge: ['gauge_id_shp', 'geometry', 'gauge_id', 'crp_pc_sse', 'wet_pc_sg1', 'for_pc_sse', 'pre_mm_syr', 'tmp_dc_syr', 'urb_pc_sse', 'lka_pc_sse', 'dis_m3_pyr', 'area', 'dominant_lc', 'dis_m3_s', 'tmp_dc_syr_C', 'mean_discharge_m3s_manual_x', 'mean_discharge_m3s_manual_y', 'mean_discharge_m3s_manual']\n",
      "Successfully found/created 'mean_discharge_m3s_manual' column.\n",
      "Total number of basins with valid manually calculated discharge: 858\n",
      "Drainage Area (km²) for basins with Q: Min=4.07, Max=16388, Median=303.8\n",
      "Mean Annual Discharge (m³/s) from daily data:\n",
      "  Min:    0.03803, Max:    204.1\n",
      "  Median: 3.596, Mean:   14.15\n",
      "Mean Annual Precipitation (mm/year) for basins with Q: Min=718, Max=1311, Median=873, Mean=879\n",
      "Mean Annual Temperature (°C) for basins with Q: Min=-18.4, Max=38.2, Median=19.0, Mean=17.7\n",
      "\n",
      "Percentage of basins (with valid Q) by dominant land cover type:\n",
      "- Agriculture: 37%\n",
      "- Forest: 35%\n",
      "- Open Water: 20%\n",
      "- Urban: 6%\n",
      "- Wetland: 1%\n",
      "\n",
      "Numerical statistics saved to: study_domain_statistics.json\n",
      "\n",
      "--- NUMERICAL VALUE COMPUTATION AND JSON EXPORT COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json # For JSON output\n",
    "\n",
    "# This code should run AFTER your \"Common Data Loading and Preparation Block\"\n",
    "# where gdf_study is created.\n",
    "\n",
    "print(\"\\n--- MANUALLY CALCULATING MEAN ANNUAL DISCHARGE & COMPUTING STATS ---\")\n",
    "\n",
    "# Dictionary to store all computed stats for JSON output\n",
    "output_stats = {}\n",
    "\n",
    "if 'gdf_study' not in globals() or gdf_study.empty:\n",
    "    print(\"ERROR: gdf_study not found or is empty. Run the common data preparation block first.\")\n",
    "    # Initialize with NaNs or empty structures if gdf_study is missing\n",
    "    output_stats['error'] = \"gdf_study not found or empty\"\n",
    "    num_studied_basins = 0\n",
    "    min_drainage_area_km2, max_drainage_area_km2, median_drainage_area_km2 = [np.nan]*3\n",
    "    min_discharge_m3s, max_discharge_m3s, median_discharge_m3s, mean_discharge_m3s = [np.nan]*4\n",
    "    min_precip_mm_yr, max_precip_mm_yr, median_precip_mm_yr, mean_precip_mm_yr = [np.nan]*4\n",
    "    min_temp_c, max_temp_c, median_temp_c, mean_temp_c = [np.nan]*4\n",
    "    dominant_lc_counts_dict = {}\n",
    "else:\n",
    "    mean_discharges_m3s_manual = {}\n",
    "    basins_with_no_valid_q_data = [] # List to collect basins with issues\n",
    "\n",
    "    print(\"Calculating mean discharge from daily CSVs...\")\n",
    "    for gauge_id_str in gdf_study['gauge_id'].unique():\n",
    "        daily_csv_path = os.path.join(CSV_FILTERED_DIR, f\"{gauge_id_str}.csv\")\n",
    "        if os.path.exists(daily_csv_path):\n",
    "            try:\n",
    "                daily_df = pd.read_csv(daily_csv_path)\n",
    "                if 'discharge' in daily_df.columns and pd.api.types.is_numeric_dtype(daily_df['discharge']):\n",
    "                    valid_discharges = daily_df['discharge'].dropna()\n",
    "                    if not valid_discharges.empty:\n",
    "                        mean_discharges_m3s_manual[gauge_id_str] = valid_discharges.mean()\n",
    "                    else:\n",
    "                        basins_with_no_valid_q_data.append(gauge_id_str) # Collect problematic basin ID\n",
    "                        mean_discharges_m3s_manual[gauge_id_str] = np.nan\n",
    "                else: # Missing 'discharge' or not numeric\n",
    "                    basins_with_no_valid_q_data.append(gauge_id_str)\n",
    "                    mean_discharges_m3s_manual[gauge_id_str] = np.nan\n",
    "            except Exception: # Catch any error during file processing\n",
    "                basins_with_no_valid_q_data.append(gauge_id_str)\n",
    "                mean_discharges_m3s_manual[gauge_id_str] = np.nan\n",
    "        else: # File not found\n",
    "            basins_with_no_valid_q_data.append(gauge_id_str)\n",
    "            mean_discharges_m3s_manual[gauge_id_str] = np.nan\n",
    "\n",
    "# ... (inside the 'else' block after the loop for manual_discharge_series calculation) ...\n",
    "\n",
    "    if basins_with_no_valid_q_data:\n",
    "        print(f\"Summary: {len(basins_with_no_valid_q_data)} basins had no valid discharge data or issues during processing.\")\n",
    "        # print(f\"Basins with issues (first 10): {basins_with_no_valid_q_data[:10]}\")\n",
    "\n",
    "    # Create the Series with the desired name\n",
    "    manual_discharge_series = pd.Series(mean_discharges_m3s_manual, name='mean_discharge_m3s_manual')\n",
    "\n",
    "    # --- CORRECTED MERGE ---\n",
    "    # Merge the series into gdf_study.\n",
    "    # The new column in gdf_study will be named after the series' name.\n",
    "    gdf_study_before_merge_cols = gdf_study.columns.tolist() # For debugging\n",
    "    gdf_study = gdf_study.merge(manual_discharge_series,\n",
    "                                left_on='gauge_id',\n",
    "                                right_index=True,\n",
    "                                how='left')\n",
    "    gdf_study_after_merge_cols = gdf_study.columns.tolist() # For debugging\n",
    "\n",
    "    print(f\"Manual mean discharge calculation complete and merged.\")\n",
    "    print(f\"Columns before merge: {gdf_study_before_merge_cols}\")\n",
    "    print(f\"Columns after merge: {gdf_study_after_merge_cols}\") # Check if 'mean_discharge_m3s_manual' is here\n",
    "\n",
    "    # Verify if the column was added with the correct name\n",
    "    if 'mean_discharge_m3s_manual' not in gdf_study.columns:\n",
    "        print(\"ERROR: 'mean_discharge_m3s_manual' column was NOT successfully added after merge.\")\n",
    "        # Check if it was added with a different name, e.g., 0 or the original Series name if it wasn't set\n",
    "        # This can happen if 'gauge_id' had issues or if the Series had no name.\n",
    "        # Our Series DOES have a name, so this shouldn't be the primary issue unless 'gauge_id' had issues.\n",
    "        # One possible fallback name pandas might use is 0 if the series name was None.\n",
    "        if 0 in gdf_study.columns and 'mean_discharge_m3s_manual' not in gdf_study_before_merge_cols:\n",
    "            print(\"A column named '0' was added. Attempting to rename it to 'mean_discharge_m3s_manual'.\")\n",
    "            gdf_study = gdf_study.rename(columns={0: 'mean_discharge_m3s_manual'})\n",
    "        elif 'mean_discharge_m3s_manual_x' in gdf_study.columns: # Pandas might add suffixes\n",
    "             print(\"Column 'mean_discharge_m3s_manual_x' found, renaming to 'mean_discharge_m3s_manual'.\")\n",
    "             gdf_study = gdf_study.rename(columns={'mean_discharge_m3s_manual_x': 'mean_discharge_m3s_manual'})\n",
    "        elif 'mean_discharge_m3s_manual_y' in gdf_study.columns:\n",
    "             print(\"Column 'mean_discharge_m3s_manual_y' found, renaming to 'mean_discharge_m3s_manual'.\")\n",
    "             gdf_study = gdf_study.rename(columns={'mean_discharge_m3s_manual_y': 'mean_discharge_m3s_manual'})\n",
    "\n",
    "    if 'mean_discharge_m3s_manual' not in gdf_study.columns:\n",
    "         print(\"CRITICAL ERROR: Still unable to find/create 'mean_discharge_m3s_manual' column. Halting statistics.\")\n",
    "         # Initialize stats with NaNs or empty to prevent further errors\n",
    "         output_stats['error'] = \"Failed to create 'mean_discharge_m3s_manual' column.\"\n",
    "         min_discharge_m3s, max_discharge_m3s, median_discharge_m3s, mean_discharge_m3s = [np.nan]*4\n",
    "         # ... initialize other stats as None/NaN ...\n",
    "    else:\n",
    "        print(\"Successfully found/created 'mean_discharge_m3s_manual' column.\")\n",
    "        # --- Now calculate all statistics (the rest of your Phase 1 code) ---\n",
    "        # This part should now work correctly as 'mean_discharge_m3s_manual' will be the column name.\n",
    "        output_stats['num_total_basins_in_gdf_study'] = len(gdf_study)\n",
    "        discharge_column_final = 'mean_discharge_m3s_manual' # This should now match\n",
    "        num_basins_with_valid_q = gdf_study[discharge_column_final].notna().sum()\n",
    "        # ... rest of your statistics calculations ...\n",
    "\n",
    "# ... (The rest of your Phase 1 script for other statistics and JSON output) ...\n",
    "    output_stats['num_basins_with_valid_q_manual'] = int(num_basins_with_valid_q) # Ensure int for JSON\n",
    "    print(f\"Total number of basins with valid manually calculated discharge: {num_basins_with_valid_q}\")\n",
    "\n",
    "\n",
    "    # 2. Drainage Area Statistics (for basins with valid Q)\n",
    "    # It might be more consistent to report area stats for the same set of basins for which Q is reported\n",
    "    gdf_for_stats = gdf_study[gdf_study[discharge_column_final].notna()].copy() # Use only basins with valid Q\n",
    "    if not gdf_for_stats.empty and 'area' in gdf_for_stats.columns:\n",
    "        min_drainage_area_km2 = gdf_for_stats['area'].min()\n",
    "        max_drainage_area_km2 = gdf_for_stats['area'].max()\n",
    "        median_drainage_area_km2 = gdf_for_stats['area'].median()\n",
    "        output_stats['drainage_area_km2'] = {\n",
    "            'min': float(min_drainage_area_km2), 'max': float(max_drainage_area_km2),\n",
    "            'median': float(median_drainage_area_km2)\n",
    "        }\n",
    "        print(f\"Drainage Area (km²) for basins with Q: Min={min_drainage_area_km2:.2f}, Max={max_drainage_area_km2:.0f}, Median={median_drainage_area_km2:.1f}\")\n",
    "    else:\n",
    "        print(\"WARNING: 'area' column not found or no basins with valid Q for area stats.\")\n",
    "        output_stats['drainage_area_km2'] = {'min': None, 'max': None, 'median': None}\n",
    "\n",
    "\n",
    "    # 3. Mean Annual Discharge Statistics (from manually calculated daily data)\n",
    "    if not gdf_for_stats.empty and discharge_column_final in gdf_for_stats.columns:\n",
    "        valid_manual_discharges = gdf_for_stats[discharge_column_final].dropna() # Should be mostly non-NaN here\n",
    "        if not valid_manual_discharges.empty:\n",
    "            min_discharge_m3s = valid_manual_discharges.min()\n",
    "            max_discharge_m3s = valid_manual_discharges.max()\n",
    "            median_discharge_m3s = valid_manual_discharges.median()\n",
    "            mean_discharge_m3s = valid_manual_discharges.mean()\n",
    "            output_stats['mean_annual_discharge_m3s'] = {\n",
    "                'min': float(min_discharge_m3s), 'max': float(max_discharge_m3s),\n",
    "                'median': float(median_discharge_m3s), 'mean': float(mean_discharge_m3s)\n",
    "            }\n",
    "            print(f\"Mean Annual Discharge (m³/s) from daily data:\")\n",
    "            print(f\"  Min:    {min_discharge_m3s:.4g}, Max:    {max_discharge_m3s:.4g}\")\n",
    "            print(f\"  Median: {median_discharge_m3s:.4g}, Mean:   {mean_discharge_m3s:.4g}\")\n",
    "        else:\n",
    "            output_stats['mean_annual_discharge_m3s'] = {'min': None, 'max': None, 'median': None, 'mean': None}\n",
    "    else:\n",
    "        print(f\"WARNING: Column '{discharge_column_final}' not found or no basins with valid Q for discharge stats.\")\n",
    "        output_stats['mean_annual_discharge_m3s'] = {'min': None, 'max': None, 'median': None, 'mean': None}\n",
    "\n",
    "\n",
    "    # 4. Mean Annual Precipitation Statistics (for basins with valid Q)\n",
    "    if not gdf_for_stats.empty and 'pre_mm_syr' in gdf_for_stats.columns:\n",
    "        min_precip_mm_yr = gdf_for_stats['pre_mm_syr'].min()\n",
    "        max_precip_mm_yr = gdf_for_stats['pre_mm_syr'].max()\n",
    "        median_precip_mm_yr = gdf_for_stats['pre_mm_syr'].median()\n",
    "        mean_precip_mm_yr = gdf_for_stats['pre_mm_syr'].mean()\n",
    "        output_stats['mean_annual_precipitation_mm_yr'] = {\n",
    "            'min': float(min_precip_mm_yr), 'max': float(max_precip_mm_yr),\n",
    "            'median': float(median_precip_mm_yr), 'mean': float(mean_precip_mm_yr)\n",
    "        }\n",
    "        print(f\"Mean Annual Precipitation (mm/year) for basins with Q: Min={min_precip_mm_yr:.0f}, Max={max_precip_mm_yr:.0f}, Median={median_precip_mm_yr:.0f}, Mean={mean_precip_mm_yr:.0f}\")\n",
    "    else:\n",
    "        print(\"WARNING: 'pre_mm_syr' column not found or no basins with valid Q for precip stats.\")\n",
    "        output_stats['mean_annual_precipitation_mm_yr'] = {'min': None, 'max': None, 'median': None, 'mean': None}\n",
    "\n",
    "\n",
    "    # 5. Mean Annual Temperature Statistics (for basins with valid Q)\n",
    "    if not gdf_for_stats.empty and 'tmp_dc_syr' in gdf_for_stats.columns and pd.api.types.is_numeric_dtype(gdf_for_stats['tmp_dc_syr']):\n",
    "        # Ensure Celsius column exists on gdf_for_stats\n",
    "        if 'tmp_dc_syr_C' not in gdf_for_stats.columns:\n",
    "             gdf_for_stats['tmp_dc_syr_C'] = (gdf_for_stats['tmp_dc_syr'] - 32) * 5/9\n",
    "\n",
    "        if 'tmp_dc_syr_C' in gdf_for_stats.columns and gdf_for_stats['tmp_dc_syr_C'].notna().any():\n",
    "            min_temp_c = gdf_for_stats['tmp_dc_syr_C'].min()\n",
    "            max_temp_c = gdf_for_stats['tmp_dc_syr_C'].max()\n",
    "            median_temp_c = gdf_for_stats['tmp_dc_syr_C'].median()\n",
    "            mean_temp_c = gdf_for_stats['tmp_dc_syr_C'].mean()\n",
    "            output_stats['mean_annual_temperature_c'] = {\n",
    "                'min': float(min_temp_c), 'max': float(max_temp_c),\n",
    "                'median': float(median_temp_c), 'mean': float(mean_temp_c)\n",
    "            }\n",
    "            print(f\"Mean Annual Temperature (°C) for basins with Q: Min={min_temp_c:.1f}, Max={max_temp_c:.1f}, Median={median_temp_c:.1f}, Mean={mean_temp_c:.1f}\")\n",
    "        else:\n",
    "            output_stats['mean_annual_temperature_c'] = {'min': None, 'max': None, 'median': None, 'mean': None}\n",
    "    else:\n",
    "        print(\"WARNING: Temp data ('tmp_dc_syr') not found/numeric or no basins with valid Q for temp stats.\")\n",
    "        output_stats['mean_annual_temperature_c'] = {'min': None, 'max': None, 'median': None, 'mean': None}\n",
    "\n",
    "\n",
    "    # 6. Land Cover Distribution (for basins with valid Q)\n",
    "    if not gdf_for_stats.empty and 'dominant_lc' in gdf_for_stats.columns and gdf_for_stats['dominant_lc'].iloc[0] != 'Not Calculated':\n",
    "        dominant_lc_counts = gdf_for_stats['dominant_lc'].value_counts(normalize=True) * 100\n",
    "        dominant_lc_counts_dict = {k: float(v) for k, v in dominant_lc_counts.items()} # Convert to dict of floats\n",
    "        output_stats['dominant_land_cover_percentage'] = dominant_lc_counts_dict\n",
    "        print(\"\\nPercentage of basins (with valid Q) by dominant land cover type:\")\n",
    "        for lc_type, percentage in dominant_lc_counts.items():\n",
    "            print(f\"- {lc_type}: {percentage:.0f}%\")\n",
    "    else:\n",
    "        print(\"WARNING: Dominant land cover not calculated or no basins with valid Q for LC stats.\")\n",
    "        output_stats['dominant_land_cover_percentage'] = {}\n",
    "\n",
    "# Save to JSON\n",
    "json_output_path = 'study_domain_statistics.json'\n",
    "try:\n",
    "    with open(json_output_path, 'w') as f:\n",
    "        json.dump(output_stats, f, indent=4, allow_nan=True) # allow_nan is important\n",
    "    print(f\"\\nNumerical statistics saved to: {json_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving statistics to JSON: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- NUMERICAL VALUE COMPUTATION AND JSON EXPORT COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
